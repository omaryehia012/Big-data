{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\spark-3.2.2-bin-hadoop3.2\n",
      "C:\\Program Files\\Java\\jdk1.8.0_161\n",
      "C:\\Users\\ascom\\anaconda3;C:\\Users\\ascom\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\ascom\\anaconda3\\Library\\usr\\bin;C:\\Users\\ascom\\anaconda3\\Library\\bin;C:\\Users\\ascom\\anaconda3\\Scripts;C:\\Program Files (x86)\\VMware\\VMware Player\\bin\\;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files (x86)\\Intel\\OpenCL SDK\\2.0\\bin\\x86;C:\\Program Files (x86)\\Intel\\OpenCL SDK\\2.0\\bin\\x64;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\150\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\;C:\\Program Files\\Azure Data Studio\\bin;C:\\Program Files\\nodejs\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\110\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\120\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\130\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\DTS\\Binn\\;C:\\Program Files (x86)\\Microsoft SQL Server\\160\\DTS\\Binn\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Java\\jdk1.8.0_161\\bin\\;C:\\Users\\ascom\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\;C:\\Users\\ascom\\AppData\\Local\\Programs\\Python\\Python310\\;C:\\Users\\ascom\\anaconda3;C:\\Users\\ascom\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\ascom\\anaconda3\\Library\\usr\\bin;C:\\Users\\ascom\\anaconda3\\Library\\bin;C:\\Users\\ascom\\anaconda3\\Scripts;C:\\Users\\ascom\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\ascom\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm Community Edition 2021.2.3\\bin;C:\\Users\\ascom\\AppData\\Local\\atom\\bin;C:\\Users\\ascom\\AppData\\Local\\GitHubDesktop\\bin;C:\\Program Files\\Azure Data Studio\\bin;C:\\Users\\ascom\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;C:\\Program Files\\heroku\\bin;C:\\Users\\ascom\\AppData\\Roaming\\npm;C:\\Users\\ascom\\.dotnet\\tools;C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\spark-3.2.2-bin-hadoop3.2\\bin;C:\\Hadoop\\bin\\bin;C:\\Program Files\\Java\\jdk1.8.0_161\\bin;C:\\kafka_2.12-3.3.1\\kafka_2.12-3.3.1\\bin\\windows;;C:\\Users\\ascom\\anaconda3\\lib\\site-packages\\numpy\\.libs\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "#spark_conf = = (conf.set(\"deploy-mode\",\"client\")\n",
    "#.set(\"spark.driver.memory\",\"20g\")\n",
    "#.set(\"spark.executor.memory\",\"20g\")\n",
    "#.set(\"spark.driver.cores\",\"4\")\n",
    "#.set(\"spark.num.executors\",\"6\")\n",
    "#.set(\"spark.executor.cores\",\"4\"))\n",
    "#sc = pyspark.SparkContext(conf=spark_conf)\n",
    "#sqlContext = SQLContext.getOrCreate(sc)\n",
    "#spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.driver.cores\",\"1\").config(\"spark.driver.memory\",\"1g\").config(\"spark.executor.cores\",\"2\") .config(\"spark.executor.memory\",\"1gb\").config(\"spark.num.executors\",\"2\").enableHiveSupport().getOrCreate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.40.1'),\n",
       " ('spark.sql.warehouse.dir', 'file:/C:/Users/ascom/spark-warehouse'),\n",
       " ('spark.driver.port', '52078'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'Python Spark SQL basic example'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.num.executors', '2'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.startTime', '1666776335369'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1666776335513'),\n",
       " ('spark.executor.memory', '1gb'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.cores', '1')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"select 'spark' as hello \")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
